---
title: "Untitled"
author: "Ran Kirshenboim"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: no
    toc: yes
    fig_width: 7
    fig_height: 4.5
    theme: cerulean
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE}

### GIVE ME CREDIT KAGGLE COMP


require(tidyverse) # for data manipulating
require(janitor) # for data cleaning
require(tidymodels) # for machine learning models
require(themis) # for down/up sampling
require(scales) # for plot scaling
require(readxl) # for reading xlsx files
require(gridExtra) # for plotting several graphics together
require(ggtext) # for facilitating text styilization
require(scorecard) # for scoring model functions
require(ggcorrplot) # for correlation plots
require(kableExtra) # for presenting model output
require(glmnet) # for lasso regression
require(ranger) # for random forest
require(kernlab) # for support vector machines


theme_set(theme_bw() +
            theme(axis.text = element_markdown(color = "grey50"),
                  axis.title = element_markdown(color = "grey50"),
                  plot.title = element_markdown(color = "grey50"),
                  legend.title = element_markdown(color = "grey50"),
                  legend.text = element_markdown(color = "grey50"),
                  plot.subtitle = element_markdown(color = "grey50",hjust = 0),
                  plot.caption = element_markdown(color = "grey50",hjust = 0),
                  panel.background = element_rect(fill = "#F4F2F6",
                                                  colour = "#F4F2F6",
                                                  size = 0.5, linetype = "solid"),
                  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                                  colour = "white"),
                  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                                  colour = "white"),
                  plot.background = element_rect(fill = "#F4F2F6"),
                  legend.background = element_rect(fill = "#F4F2F6"),
                  legend.key = element_rect(fill = "#F4F2F6")
            ))

```

# Introduction


```{r data load and preprocessing}

df_raw <- read.csv("Data_csv.csv",sep = ",")
df_raw %>%
  select(-X) %>%
  rename(N_Real_Estate_LL = NumberRealEstateLoansOrLines,
         N_Late_30_59 = NumberOfTime30.59DaysPastDueNotWorse,
         N_Late_60_89 = NumberOfTime60.89DaysPastDueNotWorse,
         N_Late_90 = NumberOfTimes90DaysLate,
         Revolving_Utilization = RevolvingUtilizationOfUnsecuredLines,
         N_Dependents = NumberOfDependents,
         N_Open_Credit_LL = NumberOfOpenCreditLinesAndLoans,
         Default = SeriousDlqin2yrs) %>%
  relocate(N_Late_30_59,.before = N_Late_60_89) %>%
  relocate(N_Late_90,.after = N_Late_60_89) %>%
  relocate(N_Open_Credit_LL,.before = N_Real_Estate_LL) %>%
  mutate(Default = as.factor(Default))-> df


```



# Exploratory database Analyisis

Exploratory data analysis (EDA) is a method of analyzing and summarizing a dataset in order to better understand its properties and characteristics. It is a critical step in the data analysis process, as it helps to identify patterns, trends, and relationships within the data that may not be immediately apparent. EDA is typically used to gain a better understanding of the data and to generate hypotheses about how the data might be used or analyzed further. It can also help to identify any problems or issues with the data, such as missing values or inconsistencies, and to determine how to address these problems. EDA is often used in conjunction with other statistical and data analysis techniques, such as hypothesis testing, regression analysis, and machine learning, to gain insights and draw conclusions from the data.

## Description of variables

Variable Name	|Description |Type
-------|--------|--------
SeriousDlqin2yrs	|Person experienced 90 days past due delinquency or worse| 	Y/N
RevolvingUtilizationOfUnsecuredLines| Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits|	percentage
age| Age of borrower in years| integer
NumberOfTime30-59DaysPastDueNotWorse| Number of times borrower has been 30-59 days past due but no worse in the last 2 years.|	integer
DebtRatio| Monthly debt payments, alimony,living costs divided by monthy gross income| percentage
MonthlyIncome	|Monthly income|	real
NumberOfOpenCreditLinesAndLoans| Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)| integer
NumberOfTimes90DaysLate	|Number of times borrower has been 90 days or more past due.|	integer
NumberRealEstateLoansOrLines	|Number of mortgage and real estate loans including home equity lines of credit|	integer
NumberOfTime60-89DaysPastDueNotWorse	|Number of times borrower has been 60-89 days past due but no worse in the last 2 years.|	integer
NumberOfDependents	|Number of dependents in family excluding themselves (spouse, children etc.)|	integer



## Exploring Missing values (NA)

It is important to look for missing values in data analysis because missing values can affect the accuracy and reliability of the results of the analysis. Missing values can occur for a variety of reasons, such as when data is not collected or recorded properly, or when data is lost or corrupted during the data collection or storage process.

If missing values are not identified and addressed, they can introduce bias into the analysis and lead to incorrect conclusions being drawn from the data. For example, if a dataset contains missing values for a particular variable, and this variable is important for the analysis, then the results of the analysis may be skewed if the missing values are not accounted for. This is because the missing values represent a group of observations that are not included in the analysis, which can lead to a biased sample and inaccurate results.

There are several methods that can be used to identify and handle missing values in data analysis, such as imputation, which involves replacing the missing values with estimates based on the available data, or exclusion, which involves removing observations with missing values from the analysis. The appropriate method will depend on the specific characteristics of the data and the goals of the analysis.

```{r}

df %>%
  mutate(across(everything(), ~ ifelse(is.na(.) == TRUE,"NA","Valid"))) %>%
  pivot_longer(cols = everything(),names_to = "Variable", values_to = "Value") %>%
  group_by(Variable,Value) %>%
  summarize(Sum = n(),.groups = "drop") %>%
  ggplot(aes(x =  reorder(Variable,Sum), y = Sum, fill = Value)) +
  geom_col(alpha = 0.85, color = "black") +
  geom_text(aes(label = paste0(round(Sum/150000,2)*100,"%")),position = position_stack( vjust = 0.5)) +
  labs(title = "**Percent of missing observations per column**",
       y = "Percent", x = element_blank()) +
  theme(axis.text.x = element_text(angle = -45,face = "bold",hjust = 0), axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  guides(fill = guide_legend(title = "**Value**"))


```

### Interpertation:

 - Most variables are complete without any missing variables
 - About 1 out of 5 observations have a missing **MonthlyIncome** variable and 3% of observations have a missing **N_Dependents** variable
 - An interesting fact is that those without a valid number of dependents do no have a valid income
 - This is why I will **exclure** these observations from the dataset as they are lacking vital information.


```{r}
df %>%
  filter(!is.na(MonthlyIncome)) -> df
```


## Univariate Analysis - Detecting abnormal values / Outliers

The database is now free of missing variable, now it is time to address any abnormal values and outliers present in different variables.

An outlier may occur due to the variability in the data, or due to experimental error/human error. They may indicate an experimental error or heavy skewness in the data(heavy-tailed distribution).

In order to detect these values, I will analyze the distribution of each of the variables using a boxplot.


```{r}
plot_boxplot <- function(df,var1) {
  
 # if (!var1 %in% c("N_Dependents","age","Revolving_Utilization"))
  var1name <- as.name(var1)
  df %>%
    ggplot(aes(x = {{var1name}})) +
    geom_boxplot(alpha = 0.75,width = 0.33,fill = "#CC8899",color = "black") +
       labs(title = paste0("Boxplot: ", {{var1}}),
         y = element_blank(), x = element_blank()) +
    theme(axis.text.y = element_blank(),axis.ticks.y = element_blank() )
}
```


```{r}
p1 <- plot_boxplot(df,names(df[2]))
p2 <- plot_boxplot(df,names(df[3]))
p3 <- plot_boxplot(df,names(df[4]))
p4 <- plot_boxplot(df,names(df[5]))

grid.arrange(p1,p2,p3,p4,ncol = 2)

p1 <- plot_boxplot(df,names(df[6]))
p2 <- plot_boxplot(df,names(df[7]))
p3 <- plot_boxplot(df,names(df[8]))
p4 <- plot_boxplot(df,names(df[9]))

grid.arrange(p1,p2,p3,p4,ncol = 2)

p1 <- plot_boxplot(df,names(df[10]))
p2 <- plot_boxplot(df,names(df[11]))

grid.arrange(p1,p2,ncol = 1)

```

### Interpertation

- The vast majority of individuals have a **Revolving Utilization Rate** which is smaller then one. I will cap it at 100%
- The average age is 50 the majority of the sample size are between 35 and 60.
We can see that there is is a person with **age** 0 which has to be an error, I will exclude him from the data set
- There are many individual with debt ratios that seems to be too high, I will exclude from the dataset all individuals with a **DebtRatio** higher then 100
- Like other variables , I will cap the **MonthlyIncome** to 100,000 to correct values with make the distribution highly skewed

<br>

- Regarding the amount of late payments, the majority of individuals do not have any late payments.We see that there are two individuals with an unusual number of times being late. I will cap these variables at 25.
- The amount of **open real-estate lines** will be capped at 10
- The amount of **Open credit lines** vary between 0 to 58. As there are many individuals with a high number of open lines I will not manipulate this variable
- The number of **dependents** will be capped at 5

<br>





```{r}
df %>%
  filter(age > 0,
         DebtRatio <= 100) %>%
  mutate(Revolving_Utilization = ifelse(Revolving_Utilization > 1,1,Revolving_Utilization),
         MonthlyIncome = ifelse(MonthlyIncome > 100000,100000,MonthlyIncome),
         across(c(N_Late_30_59,N_Late_60_89,N_Late_90),~ifelse(. > 25,25,.)),
         N_Real_Estate_LL = ifelse(N_Real_Estate_LL > 10,10,N_Real_Estate_LL),
         N_Dependents = ifelse(N_Dependents > 5,5,N_Dependents)) -> df
```

## Bivariate analysis

Now that our dataset is free of any abnormal values and outliers, I will perform a bivariate analysis to see what is the relationship between each of the predictors variables and the target variable - **Default**.

For continuous variables I will create a histogram and a density plot of each variable by default rate and for categorical variables I will create a bar plot

```{r}

plot_histogram <- function(df,var1,var2,cond = TRUE) {
  
  # From object to string: deparse(substitute(varname))
  var1name <- as.name(var1)
  
    if(var1 %in% c("DebtRatio","MonthlyIncome")) {
    # This is to avoid a log scale on null values
    df <- df %>% mutate(DebtRatio = DebtRatio + 0.0001,
                        MonthlyIncome = MonthlyIncome + 1)
  }
  
  df %>%
    ggplot(aes(x = {{var1name}},fill = {{var2}})) + 
    geom_histogram(alpha = 0.75,position = "stack",color = "black",bins = 30) +
    geom_vline(aes(xintercept = median({{var1name}})), linetype = 2,linewidth = 1) +
    labs(caption = paste0("Median ", {{var1}},
                          " is ",round(median({{df}}[[{{var1}}]]),2)),
         y = element_blank(), x = element_blank(),
         title = paste0({{var1}} ," - bivariate analysis")) +
    theme(legend.position = "none") +
    {if({{cond}}) scale_x_log10()}
}

plot_density <- function(df,var1,var2,cond = TRUE){
  
  var1name <- as.name(var1)
  
    if(var1 %in% c("DebtRatio","MonthlyIncome")) {
    # This is to avoid a log scale on null values
    df <- df %>% mutate(DebtRatio = DebtRatio + 0.0001,
                        MonthlyIncome = MonthlyIncome + 1)
  }
  
  df %>%
    ggplot(aes(x = {{var1name}},fill = {{var2}})) + 
    geom_density(alpha = 0.5,color = "black") +
    labs(y = element_blank(), x = element_blank()) +
    guides(fill = guide_legend(title = "**Default**")) +
    {if({{cond}}) scale_x_log10()} +
    {if({{cond}}) labs(caption = "*Variable is log scaled*")}
        
}

plot_bar <-  function(df,var1,var2){
  
  var1name <- as.name(var1)
  
  df %>%
    mutate({{var1name}} := as.factor({{var1name}}),
           var2 = as.factor({{var2}})) %>%
    ggplot(aes(x = {{var1name}},fill = var2)) + 
    geom_bar(position = "fill",color = "black") +
    labs(y = element_blank(), x = element_blank(),title = paste0("Default rate by ",{{var1}}," - Bivariate Analysis")) +
    guides(fill = guide_legend(title = "**Default**"))
        
}

plot_numerical <- function(df,var1,var2,cond = FALSE) {
  p1 <- plot_histogram({{df}},{{var1}},{{var2}},{{cond}})
  p2 <- plot_density({{df}},{{var1}},{{var2}},{{cond}})
  
  grid.arrange(p1,p2,ncol =2)
  
}

rm(p1,p2,p3,p4)
```


```{r, echo=FALSE, fig.height= 3,fig.width=5,fig.align='center'}
options(scipen = 100)
plot_numerical(df,names(df)[2],Default)
plot_numerical(df,names(df)[3],Default)
plot_numerical(df,names(df)[4],Default,cond = TRUE)
plot_numerical(df,names(df)[5],Default,cond = TRUE)
plot_numerical(df,names(df)[6],Default)
plot_bar(df,names(df)[7],Default)
plot_bar(df,names(df)[8],Default)
plot_bar(df,names(df)[9],Default)
plot_bar(df,names(df)[10],Default)
plot_bar(df,names(df)[11],Default)

```

### Interpertation

Some variables do not seem to have any a substantial effect on the default rate.
These are:

 - **Revolving_Utilizatation** - Individuals with higher observation rates have a higher default rate
 - **Age** - Older individuals seem to default less
 - **Debt Ratio** - Those with a higher debt ratio seems to default more
 - **Monthly Income** - Higher income seems to be associated with a lower default rate
 - **Amount of open credit line** (real estate or general) - Lower amount of credit lines seems to be correlated with a slightly lower default rate
 - **Number of times a borrower has been late** (30 to 59 days, 60 to 89 days and over 90 days) - borrowers with low amount of times being late are defaulting less.
 - **Number of dependents** - More dependents seem to translate into a slightly higher chance to default
 - Higher then average amounts of **Open Credit Lines**

## Checking for multicoliniarity

If two or more variables are highly correlated with one another, it's hard to get good estimates of their distinct effects. Although multicollinearity doesn't bias the coefficients, it does make them more unstable. Standard errors may get large, and variables that appear to have weak effects may actually have quite strong effects. 


```{r}
ggcorrplot(cor(df %>% 
                 select(-Default)),
         method = 'square', type = 'lower',lab = TRUE,digits = 1,
         colors = c("#E46726", "white", "#6D9EC1"))
```

### Interpretation

- Most predictor variables show little to no correlation with other predictor variables
- The variables that seem problematic are those who describe that number of time a borrower being late (**N_Late_30_59**,**N_Late_60_89**,**N_Late_90*). In order to resolve this problem I will:
  1. Replace these 3 predictors with a single variable indication times being late - **N_Late_X** which will take the highest value out of the 3 variables which will be removed.
  2. As saw in the bivariate analysis stage that there is a very big increase in default rates once the number of late payments is bigger then zero. For this reason I will also create a dummy variable stating if an individual was late: **N_Late_Dummy**.
  
```{r}
df %>%
  mutate(N_Late_X = pmax(N_Late_30_59,N_Late_60_89,N_Late_90),
         N_Late_Dummy = if_else(N_Late_X > 0,"1","0")) ->  df
```

## Adding Additional Variables

As being late on payment is a very strong predictor of a default in payment. I decided to study the combined effect of being late and the revolving rate, as well as being late and debt ratio.


```{r}

df %>%
  mutate(Rev_X_Late = Revolving_Utilization * as.numeric(N_Late_Dummy),
         Debt_X_Late = DebtRatio * as.numeric(N_Late_Dummy)) -> df

```


## Profile of a defaulter

Based on the analysis done in the previous stage. We can say that a defaulter has this profile:

- High **Revolving_Utilizatation** rate
- **Young**
- Higher then average **debt ratio**
- Lower then average **monthly income**
- Was **already late in payment** at least once
 
# Binning using WOE and IV

Before we can create a prediction model for credit default. We will need to 

## Calculating WOE and optimal bins

<b> What is Weight of Evidence (WOE)?</b>
The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. "Bad Customers" refers to the customers who defaulted on a loan. and "Good Customers" refers to the customers who paid back loan.

$WOE = ln(\frac{\text{Distribution of goods}}{\text{Distribution of bads}})$

<b>Where the distribution of goods</b>: refers to the percentage of good customers (those who <b>did not default</b>) in a particular group.
<b>Where the distribution of bads</b>: refers to the percentage of bad customers (those who <b>did default</b>) in a particular group.

After we calculate WOE for each group of each variable we would create bins from groups with similar WOE.

Normally, we would need to manually group together each variable by decile then bin together by hand.
Example - **revolving utilization rate**:

These are the deciles for the variable

`r quantile(df$Revolving_Utilization,probs = seq(.1, .9, by = .1))`



```{r, fig.height= 3,fig.width=5,fig.align='center'}

# quantile(df$Revolving_Utilization,probs = seq(.1, .9, by = .1))
# df %>%
#   mutate(Revolving_Utilization_decile = as.factor(ntile(Revolving_Utilization,10)),
#          decile = quantile(df$Revolving_Utilization,probs = seq(.1, .9, by = .1))) %>%
#   select(Revolving_Utilization,Revolving_Utilization_decile) %>% head(5)

df %>%
  mutate(Revolving_Utilization_decile = as.factor(ntile(Revolving_Utilization,10))) %>%
  select(Revolving_Utilization,Revolving_Utilization_decile,Default) %>% 
  ggplot(aes(x = Revolving_Utilization_decile,fill = Default)) +
  geom_bar(position = "fill") +
  labs(title = "Default rate by revolving utilization rate decile")
```

We can see that deciles 1 to 6 are quite similar. a possible way to bin this variable would using:

1. Decile 1 to 6
2. Decile 7
3. Decile 8
4. Decile 9
5. Decile 10

Luckily, R has already several packages that allow us to bin variables automaticaly, one of them being the **Scorecard** package.

<br>

Using the <i><b>scorecard::woebin</b></i> function we can bin automatically all variables by using methods including tree-like segmentation or chi-square merge.

```{r,message=FALSE, results='hide'}
woebin_plot(woebin(df, y = "Default",x = "Revolving_Utilization"),show_iv = FALSE)
```

The algorithms converge to these 4 bins for the variable **revolving utilization rate**.

for all other variables bins are:

```{r,warning=FALSE, fig.height= 3,fig.width=5,fig.align='center',message=FALSE, results='hide'}

bins <- woebin(df, y = "Default")

p = woebin_plot(bins,show_iv = FALSE)
p[2:14]

df_bin <- woebin_ply(df, bins=bins, to = "bin",print_step = 0)
df_bin <- 
  df_bin %>%
  select(-bin.1)

rm(p,bins)
```


## Calculatning information value

Information value is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula:

$IV = \sum(\text{% of non events} -\text{% of events}) * WOE$

According to Siddiqi (2006), by convention the values of the IV statistic in credit scoring can be interpreted as follows.

If the IV statistic is:

1. Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads)
2. 0.02 to 0.1, then the predictor has only a weak relationship to the Goods/Bads odds ratio
3. 0.1 to 0.3, then the predictor has a medium strength relationship to the Goods/Bads odds ratio
4. Greter then 0.3, then the predictor has a strong relationship to the Goods/Bads odds ratio.

<i> Taken in part from https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html#id-9b6a08 </i>

Using the binned dataset I have assembeled before. I will compute the information value score of each variable using the 
<i><b>scorecard::iv</b></i> function

```{r}
scorecard::iv(df_bin,y = "Default")
```

We can see that the all four created variables as well as **Revolving utilization rate** are the most influential variables and the **number of dependents** is the least influential variable.
Nevertheless, all variables have a satisfactory IV score (over 0.02) so we will not omit any variables in the modeling stage


# Modeling default rate

In this stage I will create different models to predict who will default.
I will then compere between different models in order to decide which one fits best the data

## Initial split and model preprocessing

 - In order to test different models, I will split the dataset into a two different datasets: **testing** set and **training** set
- I will also create a new variable which will act as an ID variable, this will facilitate with model comparisons 

```{r}
set.seed(2023)
split <- initial_split(df_bin %>% rowid_to_column("ID_var"), prop = 0.8, strata = "Default")
train <- training(split)
test <- testing(split)
```


- Then, as I will be using the **tidymodels** framework in the prediction stage, I will need to create a recipe to then add it to different models.

- The formula will be **predict default using all variables**

- Preprocessing steps to be added:
  1. **update** the role of ID_var to ID so it would not be taken into account when modeling
  2. transform all the bins into **dummy** variables: having either 1 or 0 as value
  3. address the fact that individuals that have defaulted are underrepresented:
  

```{r, fig.height= 3,fig.width=5,fig.align='center'}
train %>%
  count(Default) %>%
  ggplot(aes(x = Default,y = n)) +
  geom_col()
```

To correct this I will oversample those who defaulted to create a balanced dataset - this is done using the **themis** package.

```{r}


formula1 = paste(names(train)[c(1,3:12)],collapse = " + ")

default_rec1 <- 
  recipe(data = train,formula = paste0("Default ~ ",formula1,sep = "")) %>%
  update_role(ID_var, new_role = "ID") %>%
  step_dummy(all_predictors(),one_hot = TRUE) 

  # step_smote(Default)

  # step_upsample(Default, over_ratio = 1)


```

- Next is training the models on the training set and fitting it on the testing set

## Logistic Model 1: Simple model, only initial features

In the first model I will try to explain the default rate using only the variables supplied at the start of the exercise.

```{r, render = 'normal_print'}
glm_spec <- 
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

glm_model <- 
  workflow() %>% 
  add_recipe(default_rec1) %>% 
  add_model(glm_spec) %>%
  fit(data =  train)

glm_model %>%
  tidy() %>%
  kbl() %>%
  kable_classic_2(full_width = F)

glm_pred <-
  bind_cols(
    test[,"Default"],
    predict(glm_model,test),
    predict(glm_model,test,type = "prob"))

glm_pred %>% 
  conf_mat(Default, .pred_class) %>%
  autoplot(type = "heatmap")

roc_curve(glm_pred, truth = Default, estimate = .pred_0) %>% autoplot()

metric_df <-
  bind_rows(
    accuracy(glm_pred,truth = Default,.pred_class),
    roc_auc(glm_pred,truth = Default,.pred_0)) %>%
  mutate(model = "Simple LR - Base")

metric_df %>%
  kbl() %>%
  kable_classic_2(full_width = F)

rm(glm_model)
```

Our initial model has an **accuracy** score of 0.933 and and **roc_auc** score of 0.843


## Logistic Model 2: Simple model, all features

In the first model I will try to explain the default rate using all the variables (initial variables and additional variables)

```{r}

default_rec2 <- recipe(data = train, Default ~ .) %>%
  update_role(ID_var, new_role = "ID") %>%
  step_dummy(all_predictors(),one_hot = TRUE)

  # step_smote(Default)

  # step_upsample(Default, over_ratio = 1)

glm_model <- 
  workflow() %>% 
  add_recipe(default_rec2) %>% 
  add_model(glm_spec) %>%
  fit(data =  train)

glm_model %>%
  tidy() %>%
  kbl() %>%
  kable_classic_2(full_width = F)

glm2_pred <-
  bind_cols(
    test[,"Default"],
    predict(glm_model,test),
    predict(glm_model,test,type = "prob"))

glm2_pred %>% 
  conf_mat(Default, .pred_class) %>%
  autoplot(type = "heatmap")

roc_curve(glm_pred, truth = Default, estimate = .pred_0) %>% autoplot()

metric_df <-
  bind_rows(bind_rows(
    accuracy(glm_pred2,Default,.pred_class),
    roc_auc(glm_pred2,Default,.pred_0)) %>%
  mutate(model = "Simple LR - All"),
  metric_df)


metric_df %>% 
  filter(model == "Simple LR - All") %>%
  kbl() %>%
  kable_classic_2(full_width = F)

rm(glm_model,glm_spec)

```


The model with all the features is slightly better then the base model

## Penalized regression 

- Let's see if a lasso model outperforms the regular logistic regression model


```{r, eval=TRUE}
# Creating folds for cross validation
train_fold <- train %>% vfold_cv(3,strata = Default)

# Declaring the model we will use
lasso_spec <- logistic_reg(penalty = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_model <- 
  workflow() %>%
  add_recipe(default_rec2) %>%
  add_model(lasso_spec)

# Creating the specification for our tune grid
lambda_grid <- crossing(penalty = 10 ^ seq(-7,-0.5,0.25))

lasso_grid <- tune_grid(lasso_model
                        ,resamples = train_fold,
                        grid = lambda_grid,
                        control = control_grid(verbose = TRUE)
                        )

highest_acc <- lasso_grid %>% 
  select_best("roc_auc",maximise = TRUE)

lasso_grid %>% autoplot()

```

 - We correctly identified the best preforming penalty parameter, we can fit the model to the training data



```{r, eval=TRUE}

# unregister_dopar <- function() {
#   env <- foreach:::.foreachGlobals
#   rm(list=ls(name=env), pos=env)
# }
# 
# unregister_dopar()

# Applying the tuning to our workflow
lasso_model <- finalize_workflow(lasso_model,
                  highest_acc) %>% fit(data = train)

# lasso_model %>%
#   pull_workflow_fit() %>%
#   vi(lambda = highest_acc$penalty) %>%
#   mutate(Importance = abs(Importance),
#          Variable = fct_reorder(Variable, Importance),
#          Sign = fct_rev(Sign)) %>%
#   top_n(15,wt = Importance) %>%
#   ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
#   geom_col(color = "black", width = 0.8, alpha = 0.75) +
#   theme(legend.position = "none", axis.ticks.x = element_blank(),
#         axis.text.x = element_blank()) +
#    labs(title = "Most important features",subtitle = "Red bars: more chance to churn", y = element_blank())

```

The model is properly fit and it's time to use the model to predict Default.

```{r, eval=TRUE}
lasso_pred <-
  bind_cols(
    test[,"Default"],
    predict(lasso_model,test),
    predict(lasso_model,test,type = "prob"))


lasso_pred %>% 
  conf_mat(Default, .pred_class) %>%
  autoplot(type = "heatmap")

roc_curve(lasso_pred, truth = Default, estimate = .pred_0) %>% autoplot()


metric_df <-
  bind_rows(bind_rows(
    accuracy(lasso_pred,Default,.pred_class),
    roc_auc(lasso_pred,Default,.pred_0)) %>%
  mutate(model = "Lasso Regression"),
  metric_df)

metric_df %>%
  filter(model == "Lasso Regression") %>%
  kbl() %>%
  kable_classic_2(full_width = F)

rm(lasso_model,lasso_grid,lasso_spec)
```

The lasso model performs a little better then the full logistic regression model.


## Random forest

Now let's build and tune a random forest model

```{r}
rf_spec <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()) %>%
  set_mode("classification") %>% 
  set_engine(engine = "ranger")


rf_grid <-
  crossing(mtry = c(3,4),min_n = c(5,100,200),trees = c(8,16))


rf_model <- 
  workflow() %>%
  add_recipe(default_rec2) %>%
  add_model(rf_spec)


rf_tune <- tune_grid(rf_model,
          resamples = train_fold,
          grid = rf_grid,
          control = control_grid(verbose = TRUE)
          )
highest_acc <- rf_tune %>% select_best("roc_auc")


rf_tune %>% collect_metrics() %>% arrange(-mean)


rf_model <- finalize_workflow(rf_model,
                  highest_acc) %>% fit(data = train)

rf_model %>%
  pull_workflow_fit()

rf_pred <-
  bind_cols(
    test[,"Default"],
    predict(rf_model,test),
    predict(rf_model,test,type = "prob"))

rf_pred %>% 
  conf_mat(Default, .pred_class) %>%
  autoplot(type = "heatmap")

roc_curve(rf_pred, truth = Default, estimate = .pred_0) %>% autoplot()

metric_df <-
  bind_rows(bind_rows(
    accuracy(rf_pred,Default,.pred_class),
    roc_auc(rf_pred,Default,.pred_0)) %>%
  mutate(model = "Random Forest"),
  metric_df)

metric_df %>%
  filter(model == "Random Forest") %>%
  kbl() %>%
  kable_classic_2(full_width = F)

```

The **random forest** algorithm does not outperforms the penalized regression model

## Model selection and 

```{r}

```


